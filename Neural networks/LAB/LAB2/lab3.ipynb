{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ19Q_-Idwcd"
      },
      "source": [
        "# Exercise 3: Perceptron\n",
        "\n",
        "## 3.1 Perceptron for data classification\n",
        "\n",
        "In order to demonstrate the main concepts behind the perceptron, we have to define input and output data. We will use $N$ two-dimensional vectors $\\mathbf{a}_i$ as input data organized in a $2\\times N$ matrix $\\mathbf{A}$ (two rows and $N$ columns).\n",
        "\n",
        "$\\mathbf{A}=\\begin{bmatrix}\n",
        "    a_{x_1} & a_{x_2} & \\dots & a_{x_N}\\\\\n",
        "    a_{y_1} & a_{y_2} & \\dots & a_{y_N}\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Here, $N$ is the number of vectors and $a_{x_i}$, $a_{y_i}$ are the $x$ and $y$ coordinates of $i$-th vector. In this example we will demonstrate how to classify vectors in two classes. In this case, each vector can belong to only one of two possible classes, for example $C_0$ and $C_1$. Classes of each examples are defined using a matrix \\mathbf{C}, whose dimensions are $1\\times N$ defined as follows:\n",
        "\n",
        "$\\mathbf{C}=\\left[c_1, c_2, \\dots, c_N\\right]$\n",
        "\n",
        "Each element $c_i$ has value $0$ if vector $\\mathbf{a}_i$ belongs to class $C_0$ and has value $1$ if vector belongs to class $C_1$.\n",
        "\n",
        "### 3.1.1 Classification of linearly separable examples in 2D space\n",
        "\n",
        "In this experiment we will show how to use the perceptron in order to classify a vector in two linearly separable classes. We will use the following vectors as input vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3rJdrGHxdwdI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "a1=np.array([[1, 1]]).T\n",
        "a2=np.array([[1, 1]]).T\n",
        "a3=np.array([[2, 0]]).T\n",
        "a4=np.array([[1, 2]]).T\n",
        "a5=np.array([[2, 1]]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHW7zITFdwdX"
      },
      "source": [
        "Here, vectors $\\mathbf{a}_1$, $\\mathbf{a}_2$ and $\\mathbf{a}_3$ belong to class $C_0$ and other vectors belong to class $C_1$. Form the matrices $\\mathbf{A}$ and $\\mathbf{C}$ as explained. Plot the vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVcvMV9Sdwdm"
      },
      "outputs": [],
      "source": [
        "A=np.hstack([a1, a2, a3, a4, a5])\n",
        "C=np.array([[0, 0, 0, 1, 1]])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(A[0, :], A[1, :], color=[[\"red\", \"blue\"][C[0, i]] for i in range(A.shape[1])])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeNl6vKPdwdn"
      },
      "source": [
        "Vectors belonging to the same class have the same symbol in the plot. You can initialize the perceptron as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2rqWZNMcdweA"
      },
      "outputs": [],
      "source": [
        "def initp(data, labels):\n",
        "    return -0.5+np.random.rand(labels.shape[0], data.shape[0]+1)\n",
        "\n",
        "W=initp(A, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KjQD8-edweD"
      },
      "source": [
        "Here, vector $\\mathbf{W}$ is the vector with neural network weights. The first column of $\\mathbf{W}$ represents the threshold value. The hyperplane can be visualized using following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXlmTsTidweG"
      },
      "outputs": [],
      "source": [
        "def predict(W, A):\n",
        "    return (W@np.vstack([-np.ones((1, A.shape[1])), A])>=0).astype(int)\n",
        "\n",
        "def plot(W, A):\n",
        "    x_start, x_end=A[0, :].min()-1, A[0, :].max()+1\n",
        "    y_start, y_end=A[1, :].min()-1, A[1, :].max()+1\n",
        "    \n",
        "    xx, yy=np.meshgrid(np.arange(x_start, x_end, 0.01), np.arange(y_start, y_end, 0.01))\n",
        "    grid=np.vstack([xx.ravel(), yy.ravel()])\n",
        "    \n",
        "    Z=predict(W, grid).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "    plt.scatter(A[0, :], A[1, :])\n",
        "    \n",
        "    plt.scatter(A[0, :], A[1, :], color=[[\"red\", \"blue\"][C[0, i]] for i in range(A.shape[1])])\n",
        "    plt.show()\n",
        "\n",
        "plot(W, A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6tOTuOedweL"
      },
      "source": [
        "Previously initialized perceptron can be trained by calling the function *trainlms_p* until correct (or satisfactory) segmentation (division) of the plain is achieved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JazPWntwdweR"
      },
      "outputs": [],
      "source": [
        "def trainlms_p(ni, x, d, W, max_epoch):\n",
        "    w=W.copy()\n",
        "    \n",
        "    n=0\n",
        "    errors=[]\n",
        "    while (n<max_epoch):\n",
        "        n+=1\n",
        "        y=predict(w, x)\n",
        "        e=d-y\n",
        "        w+=ni*e@np.vstack([-np.ones((1, x.shape[1])), x]).T\n",
        "        error=np.sum(np.square(e))\n",
        "        errors.append(error)\n",
        "        if (error<0.02):\n",
        "            break\n",
        "    return w, errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JpRs1pIdweW"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Plot the plane and positions of last vectors with the classification plane in two cases: before and after training. Are classes $C_1$ and $C_2$ correctly separated in both cases?\n",
        "2. Show the segmentation error with regards to training iteration.\n",
        "3. Think of an experiment where you will use the perceptron to find the border in 2D space and train the required perceptron.\n",
        "4. Think of an experiment where you will use the perceptron to find the border in 3D space and train the required perceptron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXyUcfVljBQX"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "1. Before training classes are not serapted correctly. After training the classes are seprated correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXrMmn2vdwed"
      },
      "outputs": [],
      "source": [
        "ni=0.5\n",
        "max_num_iter=1000\n",
        "\n",
        "plot(W, A)\n",
        "M, errors=trainlms_p(ni, A, C, W, max_num_iter)\n",
        "plot(M, A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCZ-LljBdwej"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1, len(errors)+1), errors)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17pKTCrzdwek"
      },
      "source": [
        "### 3.1.2 Linearly inseparable case in 2D\n",
        "\n",
        "In this experiment we will try to train a perceptron for two linearly inseparable classes. To be more precise, we will try to solve the logical XOR function problem. Input vectors ai will represent the function inputs and classes $C_0$ and $C_1$ will represent the function values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AB6TSLl7dwem"
      },
      "outputs": [],
      "source": [
        "A=np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
        "C=np.array([[0, 1, 1, 0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFJJ__ehdwen"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Use the same training procedure from the first experiment. Plot the obtained results (i.e. plot the input vectors before and after the training phase in the same window). Plot the error as well.\n",
        "2. Did perceptron learn to solve the XOR problem? Explain why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gVWFaUgjo_3"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "2. Perceptron didnt learn to solve XOR problem because our training data is not linearly separable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asjVobqudwep"
      },
      "outputs": [],
      "source": [
        "W=initp(A, C)\n",
        "M, errors=trainlms_p(ni, A, C, W, max_num_iter)\n",
        "plot(M, A)\n",
        "plt.show()\n",
        "print(C)\n",
        "print(predict(M, A))\n",
        "plt.plot(range(1, len(errors)+1), errors)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAPvhS81dweq"
      },
      "source": [
        "### 3.1.3 Classification of linearly separable examples in 3D space\n",
        "\n",
        "This experiment shows how to classify examples in 3D space. Input vectors are three dimensional and belong to 2 classes which are linearly separable. Input vectors are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f1mVMWefdwet"
      },
      "outputs": [],
      "source": [
        "a1=np.array([[0, 0, 0]]).T\n",
        "a2=np.array([[0, 0, 1]]).T\n",
        "a3=np.array([[0, 1, 0]]).T\n",
        "a4=np.array([[0, 1, 1]]).T\n",
        "a5=np.array([[1, 0, 0]]).T\n",
        "\n",
        "A=np.hstack([a1, a2, a3, a4, a5])\n",
        "C=np.array([[0, 1, 0, 0, 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr1GleUvdwey"
      },
      "source": [
        "Here, vectors $\\mathbf{a}_1$, $\\mathbf{a}_3$ and $\\mathbf{a}_4$ belong to class $C_0$ and other vectors belong to class $C_1$.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Repeat the learning procedure from 3.1.1. and show the obtained results with plot of the error.\n",
        "2. Change the vector classes until classes $C_0$ and $C_1$ become linearly inseparable. When does this happen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCNy33xam9Py"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "2. It happens when we change $\\mathbf{a}_3$ vector to be classified as $C_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX_AsyqDdwez"
      },
      "outputs": [],
      "source": [
        "C=np.array([[0, 1, 0, 0, 1]])\n",
        "W=initp(A, C)\n",
        "M, errors=trainlms_p(ni, A, C, W, max_num_iter)\n",
        "\n",
        "plt.plot(range(1, len(errors)+1), errors)\n",
        "plt.show()\n",
        "\n",
        "print(predict(M, A))\n",
        "\n",
        "C=np.array([[0, 1, 1, 0, 1]])\n",
        "W=initp(A, C)\n",
        "M, errors=trainlms_p(ni, A, C, W, max_num_iter)\n",
        "\n",
        "plt.plot(range(1, len(errors)+1), errors)\n",
        "plt.show()\n",
        "\n",
        "print(predict(M, A))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOV24fFDdwe1"
      },
      "source": [
        "## 3.2 Classification of examples with Gaussian distribution\n",
        "\n",
        "The second part of this exercise tries to show how to classify examples with Gaussian distribution, which can be typically found in real life problems.\n",
        "\n",
        "Suppose we have two classes of 2D vectors, where each class represents the realization of the random vector with Gaussian distribution. We will set the mean value and standard deviation of the first class to $E(C_0)=(10, 10)$ and $S(C_0)=2.5$ for each of the components. The second class will have the expected value $E(C_1)=(20, 5)$ and standard deviation $S(C_1)=2$. Create 100 vectors for each class as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7QzQ2S1Ldwe2"
      },
      "outputs": [],
      "source": [
        "A1=np.random.normal((10, 10), (2.5, 2.5), size=(100,2)).T\n",
        "A2=np.random.normal((20, 5), (2, 2), size=(100,2)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0iCDllQdwe3"
      },
      "source": [
        "After this step we have to construct the matrix $\\mathbf{A}$ containing vectors $\\mathbf{A}_1$ and $\\mathbf{A}_2$. We have to form the vector $\\mathbf{C}$ which says that first that 100 elements belong to class $C_0$ and other elements belong to class $C_1$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M5ZI5KVWdwe6"
      },
      "outputs": [],
      "source": [
        "A=np.hstack([A1, A2])\n",
        "C=np.hstack([np.zeros((1,100)), np.ones((1, 100))]).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6bHMcQ1dwe8"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Repeat the training procedure from the first part of the exercise. Plot the obtained results.\n",
        "2. How many examples were misclassified?\n",
        "3. If the input vector is given as $\\mathbf{a}_i$=(10,3) where would we classify this example?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr7ru234dwe9"
      },
      "outputs": [],
      "source": [
        "W=initp(A, C)\n",
        "M, errors=trainlms_p(ni, A, C, W, max_num_iter)\n",
        "plot(M, A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "656_93_ydwfR"
      },
      "outputs": [],
      "source": [
        "print(\"Number of misclassified examples: %d\"%(np.sum(np.absolute(C-predict(M, A)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmyAT0GrdwfV"
      },
      "outputs": [],
      "source": [
        "a=np.array([[10], [3]])\n",
        "print(\"Classified as C%d.\"%predict(M, a)[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGdxBZdsdwfW"
      },
      "source": [
        "### 3.3.1 Classification of examples using two perceptrons\n",
        "\n",
        "The third part of the exercise shows how to use more than one perceptron in order to classify input vectors in larger number of classes. In Figure 4 we can see a network with two perceptrons which can be used in order to classify the examples in four linearly inseparable classes.\n",
        "\n",
        "![Two perceptrons](img/two.png)\n",
        "<center>Figure 4. Two perceptrons for classification in four classes (outputs are binary coded)</center>\n",
        "\n",
        "Suppose we have 10 2D input vectors defined with matrix $\\mathbf{A}$ where each column of the matrix represents one input vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e_pFcxRKdwfX"
      },
      "outputs": [],
      "source": [
        "A=np.array([[0.1, 0.7, 0.8, 0.8, 1.0, 0.3, 0.0, -0.3, -0.5, -1.5], [1.2, 1.8, 1.6, 0.6, 0.8, 0.5, 0.2, 0.8, -1.5, -1.3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukw34uoXdwfX"
      },
      "source": [
        "Matrix $\\mathbf{C}$ is used to define in which class each input vector belongs to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cqRHI0HZdwfZ"
      },
      "outputs": [],
      "source": [
        "C=np.array([[1, 1, 1, 0, 0, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgFdpyHkdwfa"
      },
      "source": [
        "Each column of the matrix $\\mathbf{C}$ is a 2D vector, where each two bits represent the binary coded class value for each input vector. Using two bits we can binary code four different values, which represent the class names: $C_0$, $C_1$, $C_2$, $C_3$. This network is trained using the same procedure used for the network with only one perceptron.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Train the network. Plot the obtained results with plot of the error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu7zk2XJdwfb"
      },
      "outputs": [],
      "source": [
        "W=initp(A, C)\n",
        "M, errors=trainlms_p(ni, A, C, W, max_num_iter)\n",
        "plt.plot(range(1, len(errors)+1), errors)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
